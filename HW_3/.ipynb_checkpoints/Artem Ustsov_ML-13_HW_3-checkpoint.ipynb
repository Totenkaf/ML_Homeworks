{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №3. Дедлайн - 21 ноября\n",
    "Основы машинного обучения. К.Шематоров  \n",
    "Группа ML-13. __Студент - Усцов Артем Алексеевич__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Service function declaration\n",
    "\n",
    "Connecting all the libraries necessary for work and declaring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 01:36:26.587291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-18 01:36:26.587324: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Main libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import keras as ks\n",
    "import keras as ks\n",
    "import nltk\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>m.kp.md</td>\n",
       "      <td>Экс-министр экономики Молдовы - главе МИДЭИ, ц...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>www.kp.by</td>\n",
       "      <td>Эта песня стала известна многим телезрителям б...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fanserials.tv</td>\n",
       "      <td>Банши 4 сезон 2 серия Бремя красоты смотреть о...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>colorbox.spb.ru</td>\n",
       "      <td>Не Беси Меня Картинки</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tula-sport.ru</td>\n",
       "      <td>В Новомосковске сыграют следж-хоккеисты алекси...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              url                                              title  \\\n",
       "0   0          m.kp.md  Экс-министр экономики Молдовы - главе МИДЭИ, ц...   \n",
       "1   1        www.kp.by  Эта песня стала известна многим телезрителям б...   \n",
       "2   2    fanserials.tv  Банши 4 сезон 2 серия Бремя красоты смотреть о...   \n",
       "3   3  colorbox.spb.ru                              Не Беси Меня Картинки   \n",
       "4   4    tula-sport.ru  В Новомосковске сыграют следж-хоккеисты алекси...   \n",
       "\n",
       "   target  \n",
       "0   False  \n",
       "1   False  \n",
       "2   False  \n",
       "3   False  \n",
       "4   False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        False\n",
       "url       False\n",
       "title     False\n",
       "target    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the empty data\n",
    "train_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135309</td>\n",
       "      <td>www.kommersant.ru</td>\n",
       "      <td>Шестой кассационный суд в Самаре начнет работу...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135310</td>\n",
       "      <td>urexpert.online</td>\n",
       "      <td>Что такое индексация алиментов, кем и в каких ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>135311</td>\n",
       "      <td>imperimeha.ru</td>\n",
       "      <td>Женщинам | Империя Меха - Part 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135312</td>\n",
       "      <td>national-porn.com</td>\n",
       "      <td>Небритые, волосатые киски: Порно всех стран и ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135313</td>\n",
       "      <td>2gis.ru</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                url  \\\n",
       "0  135309  www.kommersant.ru   \n",
       "1  135310    urexpert.online   \n",
       "2  135311      imperimeha.ru   \n",
       "3  135312  national-porn.com   \n",
       "4  135313            2gis.ru   \n",
       "\n",
       "                                               title  \n",
       "0  Шестой кассационный суд в Самаре начнет работу...  \n",
       "1  Что такое индексация алиментов, кем и в каких ...  \n",
       "2                  Женщинам | Империя Меха - Part 12  \n",
       "3  Небритые, волосатые киски: Порно всех стран и ...  \n",
       "4                                                 67  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       False\n",
       "url      False\n",
       "title    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the empty data\n",
    "test_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина вектора данных на обучении - 135309\n",
      "Длина вектора данных на тесте - 165378\n",
      "Соотношение теста к обучающим - 1.22\n"
     ]
    }
   ],
   "source": [
    "print(f\"Длина вектора данных на обучении - {len(train_df)}\")\n",
    "print(f\"Длина вектора данных на тесте - {len(test_df)}\")\n",
    "print(f\"Соотношение теста к обучающим - {round(len(test_df) / len(train_df), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    118594\n",
       "True      16715\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels balance\n",
    "train_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Simple baseline realisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = [int(\"порно\" in text) for text in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_train, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"AUC-ROC metric: {round(roc_auc_score(y_train, y_pred), 3)}\")\n",
    "# fpr, tpr, _ = roc_curve(y_train, y_pred)\n",
    "\n",
    "# plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.title('ROC curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df[\"target\"] = [(\"порно\" in text) for text in X_test]\n",
    "\n",
    "# # Create file and read in stdout\n",
    "# test_df[[\"id\", \"target\"]].to_csv(\"simple_baseline.csv\", index=False)\n",
    "# !cat simple_baseline.csv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не все так однозначно\n",
    "\n",
    "**не порно**:\n",
    "- Болезни опорно-двигательной системы и импотенция: взаимосвязь\n",
    "- Транссексуальные рыбы - National Geographic Россия: красота мира в каждом кадре\n",
    "- Групповая обзорная экскурсия по Афинам - цена €50\n",
    "- Больного раком Задорнова затравили в соцсетях.\n",
    "- Гомосексуалисты на «Первом канале»? Эрнст и Галкин – скрытая гей-пара российского шоу-бизнеса | Заметки о стиле, моде и жизни\n",
    "\n",
    "**порно**:\n",
    "- Отборная домашка\n",
    "- Сюзанна - карьера горничной / Susanna cameriera perversa (с русским переводом) 1995 г., DVDRip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. ML baseline realisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "# model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_vectorized = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_ = 42\n",
    "# print(X_train[id_])\n",
    "# x_vector = X_train_vectorized.getrow(id_).toarray()[0]\n",
    "# [feature for feature in feature_names[x_vector > 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model.fit(X_train_vectorized, y_train)\n",
    "# y_pred = model.predict(X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_train, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"AUC-ROC metric: {round(roc_auc_score(y_train, y_pred), 3)}\")\n",
    "# fpr, tpr, _ = roc_curve(y_train, y_pred)\n",
    "\n",
    "# plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.title('ROC curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_vectorized = vectorizer.transform(X_test)\n",
    "# test_df[\"target\"] = model.predict(X_test_vectorized).astype(bool)\n",
    "\n",
    "# # Create file and read in stdout\n",
    "# test_df[[\"id\", \"target\"]].to_csv(\"ml_baseline.csv\", index=False)\n",
    "# !cat ml_baseline.csv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Smart data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Имеется большой перекос в сторону классификации текста как не порносодержащего\n",
    "# train_df.groupby(\"target\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanitizing input\n",
    "Let's make sure our tweets only have characters we want. We remove '#' characters but keep the words after the '#' sign because they might be relevant (eg: #disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = codecs.open(\"train.csv\", \"r\", encoding='utf-8', errors='replace')\n",
    "# output_file = open(\"train_clean.csv\", \"w\")\n",
    "\n",
    "# def sanitize_characters(raw, clean):    \n",
    "#     for line in input_file:\n",
    "#         out = line\n",
    "#         output_file.write(line)\n",
    "\n",
    "# sanitize_characters(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train dataset\n",
    "# train_df = pd.read_csv(\"train_clean.csv\")\n",
    "# train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['title'][0] = \" \".join(ma.parse(word)[0].normal_form for word in train_df['title'][0].split())\n",
    "# # train_df['title'][0] = train_df['title'][0].encode(\"utf-8\")\n",
    "# train_df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "\n",
    "ma = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def clean_text(text, encoding=False):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) #deleting newlines and line-breaks\n",
    "    text = re.sub('[.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text) #deleting symbols  \n",
    "    text = \" \".join(ma.parse(word)[0].normal_form for word in text.split())\n",
    "    text = ' '.join(word for word in text.split() if len(word)>3)\n",
    "    if encoding:\n",
    "        text = text.encode(\"utf-8\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['title_clean'] = train_df.apply(lambda x: clean_text(x[u'title']), axis=1)\n",
    "train_df['title_clean'] = train_df['title_clean'].astype(\"str\")\n",
    "test_df['title_clean'] = test_df.apply(lambda x: clean_text(x[u'title']), axis=1).astype(\"str\")\n",
    "test_df['title_clean'] = test_df['title_clean'].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"clean_train_df.csv\")\n",
    "test_df.to_csv(\"clean_test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_train_df = pd.read_csv(\"clean_train_df.csv\")\n",
    "clean_test_df = pd.read_csv(\"clean_test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['title_clean'] = train_df.apply(lambda x: clean_text(x[u'title'], True), axis=1)\n",
    "train_df['title_clean'] = train_df['title_clean'].astype(\"str\")\n",
    "test_df['title_clean'] = test_df.apply(lambda x: clean_text(x[u'title'], True), axis=1)\n",
    "test_df['title_clean'] = test_df['title_clean'].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"clean_train_encoded_df.csv\")\n",
    "test_df.to_csv(\"clean_test_encoded_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_encoded_df = pd.read_csv(\"clean_train_encoded_df.csv\")\n",
    "clean_test_encoded_df = pd.read_csv(\"clean_test_encoded_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_train_df = clean_train_df[clean_train_df['title_clean'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# clean_train_df[\"tokens\"] = clean_train_df[\"title_clean\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# all_words = [word for tokens in clean_train_df[\"tokens\"] for word in tokens]\n",
    "# sentence_lengths = [len(tokens) for tokens in clean_train_df[\"tokens\"]]\n",
    "\n",
    "# VOCAB = sorted(list(set(all_words)))\n",
    "# print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "# print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10, 10)) \n",
    "# plt.xlabel('Sentence length')\n",
    "# plt.ylabel('Number of sentences')\n",
    "# plt.hist(sentence_lengths)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# def cv(data):\n",
    "#     count_vectorizer = CountVectorizer()\n",
    "\n",
    "#     emb = count_vectorizer.fit_transform(data)\n",
    "\n",
    "#     return emb, count_vectorizer\n",
    "\n",
    "# list_corpus = clean_train_df[\"title_clean\"].tolist()\n",
    "# list_labels = clean_train_df[\"target\"].tolist()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.30, \n",
    "#                                                                               random_state=40)\n",
    "\n",
    "# X_train_counts, count_vectorizer = cv(X_train)\n",
    "# X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA, TruncatedSVD\n",
    "# import matplotlib\n",
    "# import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "#         lsa = TruncatedSVD(n_components=2)\n",
    "#         lsa.fit(test_data)\n",
    "#         lsa_scores = lsa.transform(test_data)\n",
    "#         color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "#         color_column = [color_mapper[label] for label in test_labels]\n",
    "#         colors = ['orange','blue','blue']\n",
    "#         if plot:\n",
    "#             plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "#             red_patch = mpatches.Patch(color='orange', label='Irrelevant')\n",
    "#             green_patch = mpatches.Patch(color='blue', label='Porn')\n",
    "#             plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(16, 16))          \n",
    "# plot_LSA(X_train_counts, y_train)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "#                          multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "# clf.fit(X_train_counts, y_train)\n",
    "# y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_predicted_counts, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"AUC-ROC metric: {round(roc_auc_score(y_test, y_predicted_counts), 3)}\")\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_predicted_counts)\n",
    "\n",
    "# plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.title('ROC curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import itertools\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.winter):\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title, fontsize=30)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, fontsize=20)\n",
    "#     plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "#                  color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label', fontsize=30)\n",
    "#     plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "#     return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test, y_predicted_counts)\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "# plot = plot_confusion_matrix(cm, classes=['Irrelevant','Porn'], normalize=False, title='Confusion matrix')\n",
    "# plt.show()\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_most_important_features(vectorizer, model, n=5):\n",
    "#     index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "#     # loop for each class\n",
    "#     classes ={}\n",
    "#     for class_index in range(model.coef_.shape[0]):\n",
    "#         word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "#         sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "#         tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "#         bottom = sorted_coeff[-n:]\n",
    "\n",
    "#         classes[class_index] = { 'tops':tops, 'bottom':bottom }\n",
    "\n",
    "#     return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance = get_most_important_features(count_vectorizer, clf, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "#     y_pos = np.arange(len(top_words))\n",
    "#     top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "#     top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "#     bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "#     bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     top_words = [a[0] for a in top_pairs]\n",
    "#     top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "#     bottom_words = [a[0] for a in bottom_pairs]\n",
    "#     bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "#     fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "#     plt.subplot(121)\n",
    "#     plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "#     plt.title('Irrelevant', fontsize=20)\n",
    "#     plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "#     plt.suptitle('Key words', fontsize=16)\n",
    "#     plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "#     plt.subplot(122)\n",
    "#     plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "#     plt.title('Disaster', fontsize=20)\n",
    "#     plt.yticks(y_pos, top_words, fontsize=14)\n",
    "#     plt.suptitle(name, fontsize=16)\n",
    "#     plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "#     plt.subplots_adjust(wspace=0.8)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_scores = [a[0] for a in importance[0]['tops']]\n",
    "# top_words = [a[1] for a in importance[0]['tops']]\n",
    "# bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "# bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "\n",
    "# plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tfidf(data):\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "#     train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "#     return train, tfidf_vectorizer\n",
    "\n",
    "# X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(16, 16))          \n",
    "# plot_LSA(X_train_tfidf, y_train)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\"C\": [1, 5, 10, 30, 40, 50], \"class_weight\" : [\"balanced\"], \n",
    "#              \"solver\": [\"newton-cg\"], \"multi_class\" : [\"multinomial\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = LogisticRegression(n_jobs=-1, random_state=40)\n",
    "# clf_tfidf = GridSearchCV(estimator, param_grid, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "# #                          multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "\n",
    "# clf_tfidf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_tfidf = LogisticRegression(C=30, class_weight='balanced', solver='newton-cg', \n",
    "#                          multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "# clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_tfidf = SGDClassifier(random_state=42, class_weight='balanced',\n",
    "#                                       learning_rate = 'adaptive', eta0 = 0.1,\n",
    "#                                       loss = 'perceptron', alpha=0.00001,\n",
    "#                                       early_stopping = True, validation_fraction = 0.2,\n",
    "#                                       n_jobs = -1)\n",
    "# clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_predicted_tfidf, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"AUC-ROC metric: {round(roc_auc_score(y_test, y_predicted_tfidf), 3)}\")\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_predicted_tfidf)\n",
    "\n",
    "# plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.title('ROC curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm2 = confusion_matrix(y_test, y_predicted_tfidf)\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "# plot = plot_confusion_matrix(cm2, classes=['Irrelevant','Disaster'], normalize=False, title='Confusion matrix')\n",
    "# plt.show()\n",
    "# print(\"TFIDF confusion matrix\")\n",
    "# print(cm2)\n",
    "# print(\"BoW confusion matrix\")\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_tfidf = get_most_important_features(tfidf_vectorizer, clf_tfidf, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_scores = [a[0] for a in importance_tfidf[0]['tops']]\n",
    "# top_words = [a[1] for a in importance_tfidf[0]['tops']]\n",
    "# bottom_scores = [a[0] for a in importance_tfidf[0]['bottom']]\n",
    "# bottom_words = [a[1] for a in importance_tfidf[0]['bottom']]\n",
    "\n",
    "# plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_main = test_df[\"title\"].tolist()\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test_main)\n",
    "\n",
    "# test_df[\"target\"] =  clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create file and read in stdout\n",
    "\n",
    "# test_df[[\"id\", \"target\"]].to_csv(\"ml_tfidf.csv\", index=False)\n",
    "# !cat ml_tfidf.csv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "\n",
    "# # Load Google's pre-trained Word2Vec model.\n",
    "# word2vec_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "# word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "#     if len(tokens_list)<1:\n",
    "#         return np.zeros(k)\n",
    "#     if generate_missing:\n",
    "#         vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "#     else:\n",
    "#         vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "#     length = len(vectorized)\n",
    "#     summed = np.sum(vectorized, axis=0)\n",
    "#     averaged = np.divide(summed, length)\n",
    "#     return averaged\n",
    "\n",
    "# def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "#     embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "#                                                                                 generate_missing=generate_missing))\n",
    "#     return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = get_word2vec_embeddings(word2vec, clean_train_df)\n",
    "# X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels, \n",
    "#                                                                                         test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(16, 16))          \n",
    "# plot_LSA(embeddings, list_labels)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "#                          multi_class='multinomial', random_state=40)\n",
    "# clf_w2v.fit(X_train_word2vec, y_train_word2vec)\n",
    "# y_predicted_word2vec = clf_w2v.predict(X_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(classification_report(y_test_word2vec, y_predicted_word2vec, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"AUC-ROC metric: {round(roc_auc_score(y_test_word2vec, y_predicted_word2vec), 3)}\")\n",
    "# fpr, tpr, _ = roc_curve(y_test_word2vec, y_predicted_word2vec)\n",
    "\n",
    "# plt.plot(fpr, tpr, label=\"Simple baseline case\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.title('ROC curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_df['title_clean'] = clean_train_df['title_clean'].astype(\"str\")\n",
    "clean_test_df['title_clean'] = clean_test_df['title_clean'].astype(\"str\")\n",
    "\n",
    "descriptions = clean_train_df['title_clean']\n",
    "categories = clean_train_df['target']\n",
    "descriptions_test = clean_test_df['title_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_train_encoded_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44485/740810028.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescriptions_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_train_encoded_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcategories_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_train_encoded_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_train_encoded_df' is not defined"
     ]
    }
   ],
   "source": [
    "descriptions_encoded = clean_train_encoded_df['title_clean']\n",
    "categories_encoded = clean_train_encoded_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_test_encoded = clean_test_encoded_df['title_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    министр экономика молдова глава мидэя цель кот...\n",
       "1    этот песня стать известный многий телезритель ...\n",
       "2      банша сезон серия бремя красота смотреть онлайн\n",
       "3                                      бесить картинка\n",
       "4    новомосковск сыграть следж хоккеист алексински...\n",
       "Name: title_clean, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: target, dtype: bool"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'\\xd0\\xbc\\xd0\\xb8\\xd0\\xbd\\xd0\\xb8\\xd1\\x81\\xd1...\n",
       "1    b'\\xd1\\x8d\\xd1\\x82\\xd0\\xbe\\xd1\\x82 \\xd0\\xbf\\xd...\n",
       "2    b'\\xd0\\xb1\\xd0\\xb0\\xd0\\xbd\\xd1\\x88\\xd0\\xb0 \\xd...\n",
       "3    b'\\xd0\\xb1\\xd0\\xb5\\xd1\\x81\\xd0\\xb8\\xd1\\x82\\xd1...\n",
       "4    b'\\xd0\\xbd\\xd0\\xbe\\xd0\\xb2\\xd0\\xbe\\xd0\\xbc\\xd0...\n",
       "Name: title_clean, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: target, dtype: bool"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    шестой кассационный самара начать работа разны...\n",
       "1    такой индексация алименты какой случай произво...\n",
       "2                                 женщина империя part\n",
       "3    небритый волосатый киска порно весь страна нац...\n",
       "4                                                  NaN\n",
       "Name: title_clean, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'\\xd1\\x88\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xbe\\xd0...\n",
       "1    b'\\xd1\\x82\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb9 \\xd...\n",
       "2    b'\\xd0\\xb6\\xd0\\xb5\\xd0\\xbd\\xd1\\x89\\xd0\\xb8\\xd0...\n",
       "3    b'\\xd0\\xbd\\xd0\\xb5\\xd0\\xb1\\xd1\\x80\\xd0\\xb8\\xd1...\n",
       "4                                                  b''\n",
       "Name: title_clean, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions_test_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем единый словарь (слово -> число) для преобразования\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(descriptions.tolist())\n",
    "\n",
    "# Преобразуем все описания в числовые последовательности, заменяя слова на числа по словарю.\n",
    "textSequences = tokenizer.texts_to_sequences(descriptions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 27062\n",
      "\n",
      "Training set:\n",
      "\t - x_train: 108247\n",
      "\t - y_train: 108247\n",
      "\n",
      "Testing set:\n",
      "\t - x_test: 27062\n",
      "\t - y_test: 27062\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_data_from_arrays(textSequences, categories, train_test_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В словаре 122243 слов\n"
     ]
    }
   ],
   "source": [
    "total_words = len(tokenizer.word_index)\n",
    "print('В словаре {} слов'.format(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# количество наиболее часто используемых слов\n",
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Преобразуем описания заявок в векторы чисел...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44485/3353716017.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Преобразуем описания заявок в векторы чисел...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(u'Преобразуем описания заявок в векторы чисел...')\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "X_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(X_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество категорий для классификации: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print('Количество категорий для классификации: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность X_train: (108247, 10000)\n",
      "Размерность X_test: (27062, 10000)\n",
      "Преобразуем категории в матрицу двоичных чисел (для использования categorical_crossentropy)\n",
      "y_train shape: (108247, 2)\n",
      "y_test shape: (27062, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Размерность X_train:', X_train.shape)\n",
    "print('Размерность X_test:', X_test.shape)\n",
    "\n",
    "print(u'Преобразуем категории в матрицу двоичных чисел '\n",
    "      u'(для использования categorical_crossentropy)')\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собираем модель...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 512)               5120512   \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,121,538\n",
      "Trainable params: 5,121,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# количество эпох\\итераций для обучения\n",
    "epochs = 7\n",
    "\n",
    "print(u'Собираем модель...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(num_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 01:13:00.165361: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4329880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "3383/3383 [==============================] - 60s 18ms/step - loss: 0.0538 - accuracy: 0.9831\n",
      "Epoch 2/7\n",
      "3383/3383 [==============================] - 59s 18ms/step - loss: 0.0282 - accuracy: 0.9910\n",
      "Epoch 3/7\n",
      "3383/3383 [==============================] - 58s 17ms/step - loss: 0.0218 - accuracy: 0.9930\n",
      "Epoch 4/7\n",
      "3383/3383 [==============================] - 55s 16ms/step - loss: 0.0182 - accuracy: 0.9941\n",
      "Epoch 5/7\n",
      "3383/3383 [==============================] - 55s 16ms/step - loss: 0.0162 - accuracy: 0.9948\n",
      "Epoch 6/7\n",
      "3383/3383 [==============================] - 58s 17ms/step - loss: 0.0148 - accuracy: 0.9953\n",
      "Epoch 7/7\n",
      "3383/3383 [==============================] - 56s 17ms/step - loss: 0.0138 - accuracy: 0.9957\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(\"model_normal.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_normal.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_df = pd.read_csv(\"clean_train_df.csv\")\n",
    "clean_test_df = pd.read_csv(\"clean_test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_df['title_clean'] = clean_train_df['title_clean'].astype(\"str\")\n",
    "clean_test_df['title_clean'] = clean_test_df['title_clean'].astype(\"str\")\n",
    "\n",
    "descriptions = clean_train_df['title_clean']\n",
    "descriptions_test = clean_test_df['title_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 01:34:53.925209: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-18 01:34:53.925245: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-18 01:34:53.925268: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tater-VivoBook-15-ASUS-Laptop-X542UF): /proc/driver/nvidia/version does not exist\n",
      "2021-11-18 01:34:53.925592: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model_normal.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_normal.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = loaded_model.evaluate(X_test, y_test,\n",
    "#                        batch_size=32, verbose=1)\n",
    "# print()\n",
    "# print(u'Оценка теста: {}'.format(score[0]))\n",
    "# print(u'Оценка точности модели: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # График точности модели\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# # plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # График оценки loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# # plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.evaluate(X_test, y_test,\n",
    "#                        batch_size=32, verbose=1)\n",
    "# print()d\n",
    "# print(u'Оценка теста: {}'.format(score[0]))\n",
    "# print(u'Оценка точности модели: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем единый словарь (слово -> число) для преобразования\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(descriptions_test.tolist())\n",
    "\n",
    "# Преобразуем все описания в числовые последовательности, заменяя слова на числа по словарю.\n",
    "textSequences = tokenizer.texts_to_sequences(descriptions_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 16538\n",
      "\n",
      "Training set:\n",
      "\t - x_train: 148840\n",
      "\t - y_train: 118771\n",
      "\n",
      "Testing set:\n",
      "\t - x_test: 16538\n",
      "\t - y_test: 16538\n"
     ]
    }
   ],
   "source": [
    "X_train_, y_train_, X_test_, y_test_ = load_data_from_arrays(textSequences, categories, train_test_split=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "X_test_ = tokenizer.sequences_to_matrix(X_test_, mode='binary')\n",
    "X_train_ = tokenizer.sequences_to_matrix(X_train_, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_NN_predict_1 = loaded_model.predict(X_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_NN_predict_2 = loaded_model.predict(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lablels = []\n",
    "for i in range(len(y_NN_predict_1)):\n",
    "    predicted_lablels.append(np.argmax(y_NN_predict_1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108247"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_lablels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_NN_predict_2)):\n",
    "    predicted_lablels.append(np.argmax(y_NN_predict_2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135309"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_lablels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lablels = np.array(predicted_lablels)\n",
    "predicted_lablels = np.where(predicted_lablels == 0, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "c = collections.Counter()\n",
    "for label in predicted_lablels:\n",
    "    c[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({False: 119171, True: 16138})\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (135309) does not match length of index (165378)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44184/746804939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_test_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_lablels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create file and read in stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclean_test_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ml_network.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat ml_baseline.csv | head'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3237\u001b[0m         \"\"\"\n\u001b[1;32m   3238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3239\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3240\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3895\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3896\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \"\"\"\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    752\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (135309) does not match length of index (165378)"
     ]
    }
   ],
   "source": [
    "clean_test_df[\"target\"] = predicted_lablels\n",
    "\n",
    "# Create file and read in stdout\n",
    "clean_test_df[[\"id\", \"target\"]].to_csv(\"ml_network.csv\", index=False)\n",
    "!cat ml_baseline.csv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44184/3419207291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165328</th>\n",
       "      <td>300637</td>\n",
       "      <td>vapebx.com</td>\n",
       "      <td>Купить RDA дрипка для вейпа в России | VapeBX</td>\n",
       "      <td>купить дрипка вейп россия vapebx</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165329</th>\n",
       "      <td>300638</td>\n",
       "      <td>regpovestka.ru</td>\n",
       "      <td>Ученые из Томска разработали новые импланты дл...</td>\n",
       "      <td>учёный томск разработать новый имплант позвоно...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165330</th>\n",
       "      <td>300639</td>\n",
       "      <td>www.gismeteo.ru</td>\n",
       "      <td>GISMETEO: погода в Верхнеманчарово сегодня ― п...</td>\n",
       "      <td>gismeteo погода верхнеманчарово сегодня прогно...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165331</th>\n",
       "      <td>300640</td>\n",
       "      <td>mail.ru</td>\n",
       "      <td>letty black porno 2019 - 10 роликов. Поиск Mai...</td>\n",
       "      <td>letty black porno ролик поиск mail</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165332</th>\n",
       "      <td>300641</td>\n",
       "      <td>xinkaloff.com</td>\n",
       "      <td>Хачапури по-аджарски</td>\n",
       "      <td>хачапури аджарски</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165333</th>\n",
       "      <td>300642</td>\n",
       "      <td>spb.rabota.ru</td>\n",
       "      <td>Переписка с работодателем — Rabota.ru</td>\n",
       "      <td>переписка работодатель rabota</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165334</th>\n",
       "      <td>300643</td>\n",
       "      <td>24eropixel.net</td>\n",
       "      <td>Восемнадцатилетняя Мария снимается для эротиче...</td>\n",
       "      <td>восемнадцатилетний мария сниматься эротический...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165335</th>\n",
       "      <td>300644</td>\n",
       "      <td>aim.uz</td>\n",
       "      <td>Buyruq gap . (95 –97-mashqlar )</td>\n",
       "      <td>buyruq mashqlar</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165336</th>\n",
       "      <td>300645</td>\n",
       "      <td>mediametrics.ru</td>\n",
       "      <td>pos-2;http://www.astrakhan-24.ru/news/Incident...</td>\n",
       "      <td>http astrakhan news incidents sotrudniki astra...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165337</th>\n",
       "      <td>300646</td>\n",
       "      <td>artfile.me</td>\n",
       "      <td>Картинки 3D графики на рабочий стол, скачать о...</td>\n",
       "      <td>картинка графика рабочий стол скачать обои дал...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165338</th>\n",
       "      <td>300647</td>\n",
       "      <td>sevastopol.cian.ru</td>\n",
       "      <td>Сдам однокомнатную квартиру 25м² ул. Пожарова,...</td>\n",
       "      <td>сдать однокомнатный квартира пожаров севастопо...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165339</th>\n",
       "      <td>300648</td>\n",
       "      <td>m.ok.ru</td>\n",
       "      <td>Главная страница друга</td>\n",
       "      <td>главный страница друг</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165340</th>\n",
       "      <td>300649</td>\n",
       "      <td>havepussy.com</td>\n",
       "      <td>Сиськи, страница 9 » Эротические фото и порно ...</td>\n",
       "      <td>сиська страница эротический фото порно картинк...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165341</th>\n",
       "      <td>300650</td>\n",
       "      <td>mamba.ua</td>\n",
       "      <td>Знакомства Mamba Украина — Хороший Парень, Укр...</td>\n",
       "      <td>знакомство mamba украина хороший парень украин...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165342</th>\n",
       "      <td>300651</td>\n",
       "      <td>booksbunker.com</td>\n",
       "      <td>Месть смотрящего — Экшен — Детективы — Читать ...</td>\n",
       "      <td>месть смотреть экшен детектив читать онлайн bo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165343</th>\n",
       "      <td>300652</td>\n",
       "      <td>vladimir.cian.ru</td>\n",
       "      <td>Снять однокомнатную квартиру 40м² Студенческая...</td>\n",
       "      <td>снять однокомнатный квартира студенческий влад...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165344</th>\n",
       "      <td>300653</td>\n",
       "      <td>4brain.ru</td>\n",
       "      <td>Лидерство и взаимоотношения | Блог 4brain - Pa...</td>\n",
       "      <td>лидерство взаимоотношение блог brain part</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165345</th>\n",
       "      <td>300654</td>\n",
       "      <td>www.consultant.ru</td>\n",
       "      <td>ГК РФ Статья 1406.1. Ответственность за наруше...</td>\n",
       "      <td>статья ответственность нарушение исключительны...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165346</th>\n",
       "      <td>300655</td>\n",
       "      <td>www.gismeteo.ru</td>\n",
       "      <td>GISMETEO: погода в Гелиболу по часам — подробн...</td>\n",
       "      <td>gismeteo погода гелибола подробный прогноз пог...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165347</th>\n",
       "      <td>300656</td>\n",
       "      <td>mail.ru</td>\n",
       "      <td>Мой Мир@Mail.Ru: Поиск пользователей</td>\n",
       "      <td>mail поиск пользователь</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165348</th>\n",
       "      <td>300657</td>\n",
       "      <td>orsk.ru</td>\n",
       "      <td>Назначен руководитель орского авиаклуба «Стриж...</td>\n",
       "      <td>назначить руководитель орский авиаклуб «стрижи...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165349</th>\n",
       "      <td>300658</td>\n",
       "      <td>happylook.ru</td>\n",
       "      <td>Оправы / Каталог товаров / Салоны оптики \"Счас...</td>\n",
       "      <td>оправа каталог товар салон оптика счастливый в...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165350</th>\n",
       "      <td>300659</td>\n",
       "      <td>instrukzii.ru</td>\n",
       "      <td>Должностная инструкция охранника 6-го разряда</td>\n",
       "      <td>должностной инструкция охранник разряд</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165351</th>\n",
       "      <td>300660</td>\n",
       "      <td>www.silverlife.ru</td>\n",
       "      <td>Серебряное кольцо ALEXANDRE VASSILIEV с розовы...</td>\n",
       "      <td>серебряный кольцо alexandre vassiliev розовый ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165352</th>\n",
       "      <td>300661</td>\n",
       "      <td>mail.ru</td>\n",
       "      <td>погода в п христофорово кировской области лузс...</td>\n",
       "      <td>погода христофоровый кировский область лузский...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165353</th>\n",
       "      <td>300662</td>\n",
       "      <td>mail.ru</td>\n",
       "      <td>(7) Мой Мир@Mail.Ru</td>\n",
       "      <td>mail</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165354</th>\n",
       "      <td>300663</td>\n",
       "      <td>omsk.positronica.ru</td>\n",
       "      <td>Мой заказ №175585_1</td>\n",
       "      <td>заказ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165355</th>\n",
       "      <td>300664</td>\n",
       "      <td>dic.academic.ru</td>\n",
       "      <td>рулевая колонка - это... Что такое рулевая кол...</td>\n",
       "      <td>рулевой колонка такой рулевой колонка</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165356</th>\n",
       "      <td>300665</td>\n",
       "      <td>manga-online.biz</td>\n",
       "      <td>Читать мангу Правитель 159 глава на русском он...</td>\n",
       "      <td>читать манга правитель глава русский онлайн</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165357</th>\n",
       "      <td>300666</td>\n",
       "      <td>songspro.ru</td>\n",
       "      <td>Текст песни А.Манджиев - Ээж мини перевод, сло...</td>\n",
       "      <td>текст песня манджий минь перевод слово песня в...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165358</th>\n",
       "      <td>300667</td>\n",
       "      <td>flagma.ru</td>\n",
       "      <td>Расстояние Таганрог Казань, 1580 км, +2 маршрута</td>\n",
       "      <td>расстояние таганрог казань маршрут</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165359</th>\n",
       "      <td>300668</td>\n",
       "      <td>mledy.ru</td>\n",
       "      <td>Трендовые детские нарядные платья для девочек ...</td>\n",
       "      <td>трендовый детский нарядный платье девочка клас...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165360</th>\n",
       "      <td>300669</td>\n",
       "      <td>www.drive2.ru</td>\n",
       "      <td>Устранил бряконье передних колодок — Mitsubish...</td>\n",
       "      <td>устранить бряконья передний колодка mitsubishi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165361</th>\n",
       "      <td>300670</td>\n",
       "      <td>tvil.ru</td>\n",
       "      <td>Заявки на бронь</td>\n",
       "      <td>заявка бронь</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165362</th>\n",
       "      <td>300671</td>\n",
       "      <td>olx.uz</td>\n",
       "      <td>Участка сотилади: 50 000 у.е. - Продажа Бухара...</td>\n",
       "      <td>участок сотиладь быть продажа бухара</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165363</th>\n",
       "      <td>300672</td>\n",
       "      <td>www.olx.ua</td>\n",
       "      <td>Фото динамо 1980 года с афтографами: 100 $ - К...</td>\n",
       "      <td>фото динамо афтограф коллекционирование киев</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165364</th>\n",
       "      <td>300673</td>\n",
       "      <td>m.sima-land.ru</td>\n",
       "      <td>Канистра пищевая «Бочонок», 10 л, со сливом, б...</td>\n",
       "      <td>канистра пищевой «бочонок» слив белый купить ц...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165365</th>\n",
       "      <td>300674</td>\n",
       "      <td>ancensored.com</td>\n",
       "      <td>Nicole Sheridan nude pics, page - 3 &lt; ANCENSORED</td>\n",
       "      <td>nicole sheridan nude pics page ancensored</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165366</th>\n",
       "      <td>300675</td>\n",
       "      <td>maminshop.ru</td>\n",
       "      <td>Добро и зло в русско народных сказках. Конспек...</td>\n",
       "      <td>добро русско народный сказка конспект тематиче...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165367</th>\n",
       "      <td>300676</td>\n",
       "      <td>z-aya.ru</td>\n",
       "      <td>Дочь Ирины Пеговой ищет мужа своей маме</td>\n",
       "      <td>дочь ирина пеговой искать свой мама</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165368</th>\n",
       "      <td>300677</td>\n",
       "      <td>ancensored.com</td>\n",
       "      <td>Naked Kelsi Monroe in Pussy Portraits &lt; ANCENS...</td>\n",
       "      <td>naked kelsi monroe pussy portraits ancensored</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165369</th>\n",
       "      <td>300678</td>\n",
       "      <td>www.russhanson.org</td>\n",
       "      <td>Беломорканал - Жиганчики донские - слушать пес...</td>\n",
       "      <td>беломорканал жиганчик донский слушать песня он...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165370</th>\n",
       "      <td>300679</td>\n",
       "      <td>rusvideos.tv</td>\n",
       "      <td>Русское порно с блондинками – эти девахи точно...</td>\n",
       "      <td>русский порно блондинка этот деваха точно знат...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165371</th>\n",
       "      <td>300680</td>\n",
       "      <td>37.1.207.65</td>\n",
       "      <td>new-rutor.org :: FB2 :: бесплатный торрент ска...</td>\n",
       "      <td>rutor бесплатный торрент скачать бесплатно rut...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165372</th>\n",
       "      <td>300681</td>\n",
       "      <td>forum.ru-board.com</td>\n",
       "      <td>TeamViewer (часть 1) - [16] :: Программы :: Ко...</td>\n",
       "      <td>teamviewer часть программа компьютерный форум ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165373</th>\n",
       "      <td>300682</td>\n",
       "      <td>etp.armtek.ru</td>\n",
       "      <td>Armtek - запчасти для грузовых и легковых авто...</td>\n",
       "      <td>armtek запчасть грузовой легковой автомобиль о...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165374</th>\n",
       "      <td>300683</td>\n",
       "      <td>mail.ru</td>\n",
       "      <td>Лилия Якупова - Караганда, Карагандинская обла...</td>\n",
       "      <td>лилия якупов караганда карагандинский область ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165375</th>\n",
       "      <td>300684</td>\n",
       "      <td>xn----8sbnqchpeeeth.xn--p1ai</td>\n",
       "      <td>Администрация Лесного района Тверской области ...</td>\n",
       "      <td>администрация лесной район тверской область го...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165376</th>\n",
       "      <td>300685</td>\n",
       "      <td>www-sunhome-ru.cdn.ampproject.org</td>\n",
       "      <td>Сонник Изменение сознания. К чему снится Измен...</td>\n",
       "      <td>сонник изменение сознание сниться изменение со...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165377</th>\n",
       "      <td>300686</td>\n",
       "      <td>virtual-hockey.org</td>\n",
       "      <td>Строительство базы команды Гранд-Рапидс Гриффи...</td>\n",
       "      <td>строительство база команда гранд рапидс гриффи...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                url  \\\n",
       "165328  300637                         vapebx.com   \n",
       "165329  300638                     regpovestka.ru   \n",
       "165330  300639                    www.gismeteo.ru   \n",
       "165331  300640                            mail.ru   \n",
       "165332  300641                      xinkaloff.com   \n",
       "165333  300642                      spb.rabota.ru   \n",
       "165334  300643                     24eropixel.net   \n",
       "165335  300644                             aim.uz   \n",
       "165336  300645                    mediametrics.ru   \n",
       "165337  300646                         artfile.me   \n",
       "165338  300647                 sevastopol.cian.ru   \n",
       "165339  300648                            m.ok.ru   \n",
       "165340  300649                      havepussy.com   \n",
       "165341  300650                           mamba.ua   \n",
       "165342  300651                    booksbunker.com   \n",
       "165343  300652                   vladimir.cian.ru   \n",
       "165344  300653                          4brain.ru   \n",
       "165345  300654                  www.consultant.ru   \n",
       "165346  300655                    www.gismeteo.ru   \n",
       "165347  300656                            mail.ru   \n",
       "165348  300657                            orsk.ru   \n",
       "165349  300658                       happylook.ru   \n",
       "165350  300659                      instrukzii.ru   \n",
       "165351  300660                  www.silverlife.ru   \n",
       "165352  300661                            mail.ru   \n",
       "165353  300662                            mail.ru   \n",
       "165354  300663                omsk.positronica.ru   \n",
       "165355  300664                    dic.academic.ru   \n",
       "165356  300665                   manga-online.biz   \n",
       "165357  300666                        songspro.ru   \n",
       "165358  300667                          flagma.ru   \n",
       "165359  300668                           mledy.ru   \n",
       "165360  300669                      www.drive2.ru   \n",
       "165361  300670                            tvil.ru   \n",
       "165362  300671                             olx.uz   \n",
       "165363  300672                         www.olx.ua   \n",
       "165364  300673                     m.sima-land.ru   \n",
       "165365  300674                     ancensored.com   \n",
       "165366  300675                       maminshop.ru   \n",
       "165367  300676                           z-aya.ru   \n",
       "165368  300677                     ancensored.com   \n",
       "165369  300678                 www.russhanson.org   \n",
       "165370  300679                       rusvideos.tv   \n",
       "165371  300680                        37.1.207.65   \n",
       "165372  300681                 forum.ru-board.com   \n",
       "165373  300682                      etp.armtek.ru   \n",
       "165374  300683                            mail.ru   \n",
       "165375  300684       xn----8sbnqchpeeeth.xn--p1ai   \n",
       "165376  300685  www-sunhome-ru.cdn.ampproject.org   \n",
       "165377  300686                 virtual-hockey.org   \n",
       "\n",
       "                                                    title  \\\n",
       "165328      Купить RDA дрипка для вейпа в России | VapeBX   \n",
       "165329  Ученые из Томска разработали новые импланты дл...   \n",
       "165330  GISMETEO: погода в Верхнеманчарово сегодня ― п...   \n",
       "165331  letty black porno 2019 - 10 роликов. Поиск Mai...   \n",
       "165332                               Хачапури по-аджарски   \n",
       "165333              Переписка с работодателем — Rabota.ru   \n",
       "165334  Восемнадцатилетняя Мария снимается для эротиче...   \n",
       "165335                    Buyruq gap . (95 –97-mashqlar )   \n",
       "165336  pos-2;http://www.astrakhan-24.ru/news/Incident...   \n",
       "165337  Картинки 3D графики на рабочий стол, скачать о...   \n",
       "165338  Сдам однокомнатную квартиру 25м² ул. Пожарова,...   \n",
       "165339                             Главная страница друга   \n",
       "165340  Сиськи, страница 9 » Эротические фото и порно ...   \n",
       "165341  Знакомства Mamba Украина — Хороший Парень, Укр...   \n",
       "165342  Месть смотрящего — Экшен — Детективы — Читать ...   \n",
       "165343  Снять однокомнатную квартиру 40м² Студенческая...   \n",
       "165344  Лидерство и взаимоотношения | Блог 4brain - Pa...   \n",
       "165345  ГК РФ Статья 1406.1. Ответственность за наруше...   \n",
       "165346  GISMETEO: погода в Гелиболу по часам — подробн...   \n",
       "165347               Мой Мир@Mail.Ru: Поиск пользователей   \n",
       "165348  Назначен руководитель орского авиаклуба «Стриж...   \n",
       "165349  Оправы / Каталог товаров / Салоны оптики \"Счас...   \n",
       "165350      Должностная инструкция охранника 6-го разряда   \n",
       "165351  Серебряное кольцо ALEXANDRE VASSILIEV с розовы...   \n",
       "165352  погода в п христофорово кировской области лузс...   \n",
       "165353                                (7) Мой Мир@Mail.Ru   \n",
       "165354                                Мой заказ №175585_1   \n",
       "165355  рулевая колонка - это... Что такое рулевая кол...   \n",
       "165356  Читать мангу Правитель 159 глава на русском он...   \n",
       "165357  Текст песни А.Манджиев - Ээж мини перевод, сло...   \n",
       "165358   Расстояние Таганрог Казань, 1580 км, +2 маршрута   \n",
       "165359  Трендовые детские нарядные платья для девочек ...   \n",
       "165360  Устранил бряконье передних колодок — Mitsubish...   \n",
       "165361                                    Заявки на бронь   \n",
       "165362  Участка сотилади: 50 000 у.е. - Продажа Бухара...   \n",
       "165363  Фото динамо 1980 года с афтографами: 100 $ - К...   \n",
       "165364  Канистра пищевая «Бочонок», 10 л, со сливом, б...   \n",
       "165365   Nicole Sheridan nude pics, page - 3 < ANCENSORED   \n",
       "165366  Добро и зло в русско народных сказках. Конспек...   \n",
       "165367            Дочь Ирины Пеговой ищет мужа своей маме   \n",
       "165368  Naked Kelsi Monroe in Pussy Portraits < ANCENS...   \n",
       "165369  Беломорканал - Жиганчики донские - слушать пес...   \n",
       "165370  Русское порно с блондинками – эти девахи точно...   \n",
       "165371  new-rutor.org :: FB2 :: бесплатный торрент ска...   \n",
       "165372  TeamViewer (часть 1) - [16] :: Программы :: Ко...   \n",
       "165373  Armtek - запчасти для грузовых и легковых авто...   \n",
       "165374  Лилия Якупова - Караганда, Карагандинская обла...   \n",
       "165375  Администрация Лесного района Тверской области ...   \n",
       "165376  Сонник Изменение сознания. К чему снится Измен...   \n",
       "165377  Строительство базы команды Гранд-Рапидс Гриффи...   \n",
       "\n",
       "                                              title_clean  target  \n",
       "165328                   купить дрипка вейп россия vapebx   False  \n",
       "165329  учёный томск разработать новый имплант позвоно...   False  \n",
       "165330  gismeteo погода верхнеманчарово сегодня прогно...   False  \n",
       "165331                 letty black porno ролик поиск mail   False  \n",
       "165332                                  хачапури аджарски   False  \n",
       "165333                      переписка работодатель rabota   False  \n",
       "165334  восемнадцатилетний мария сниматься эротический...   False  \n",
       "165335                                    buyruq mashqlar   False  \n",
       "165336  http astrakhan news incidents sotrudniki astra...   False  \n",
       "165337  картинка графика рабочий стол скачать обои дал...   False  \n",
       "165338  сдать однокомнатный квартира пожаров севастопо...   False  \n",
       "165339                              главный страница друг   False  \n",
       "165340  сиська страница эротический фото порно картинк...    True  \n",
       "165341  знакомство mamba украина хороший парень украин...   False  \n",
       "165342  месть смотреть экшен детектив читать онлайн bo...   False  \n",
       "165343  снять однокомнатный квартира студенческий влад...   False  \n",
       "165344          лидерство взаимоотношение блог brain part   False  \n",
       "165345  статья ответственность нарушение исключительны...   False  \n",
       "165346  gismeteo погода гелибола подробный прогноз пог...   False  \n",
       "165347                            mail поиск пользователь   False  \n",
       "165348  назначить руководитель орский авиаклуб «стрижи...   False  \n",
       "165349  оправа каталог товар салон оптика счастливый в...   False  \n",
       "165350             должностной инструкция охранник разряд   False  \n",
       "165351  серебряный кольцо alexandre vassiliev розовый ...   False  \n",
       "165352  погода христофоровый кировский область лузский...   False  \n",
       "165353                                               mail   False  \n",
       "165354                                              заказ   False  \n",
       "165355              рулевой колонка такой рулевой колонка   False  \n",
       "165356        читать манга правитель глава русский онлайн   False  \n",
       "165357  текст песня манджий минь перевод слово песня в...   False  \n",
       "165358                 расстояние таганрог казань маршрут   False  \n",
       "165359  трендовый детский нарядный платье девочка клас...   False  \n",
       "165360  устранить бряконья передний колодка mitsubishi...   False  \n",
       "165361                                       заявка бронь   False  \n",
       "165362               участок сотиладь быть продажа бухара    True  \n",
       "165363       фото динамо афтограф коллекционирование киев   False  \n",
       "165364  канистра пищевой «бочонок» слив белый купить ц...    True  \n",
       "165365          nicole sheridan nude pics page ancensored   False  \n",
       "165366  добро русско народный сказка конспект тематиче...   False  \n",
       "165367                дочь ирина пеговой искать свой мама   False  \n",
       "165368      naked kelsi monroe pussy portraits ancensored   False  \n",
       "165369  беломорканал жиганчик донский слушать песня он...   False  \n",
       "165370  русский порно блондинка этот деваха точно знат...   False  \n",
       "165371  rutor бесплатный торрент скачать бесплатно rut...   False  \n",
       "165372  teamviewer часть программа компьютерный форум ...    True  \n",
       "165373  armtek запчасть грузовой легковой автомобиль о...   False  \n",
       "165374  лилия якупов караганда карагандинский область ...   False  \n",
       "165375  администрация лесной район тверской область го...   False  \n",
       "165376  сонник изменение сознание сниться изменение со...   False  \n",
       "165377  строительство база команда гранд рапидс гриффи...   False  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# создаем единый словарь (слово -> число) для преобразования\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(descriptions.tolist())\n",
    "\n",
    "# Преобразуем все описания в числовые последовательности, заменяя слова на числа по словарю.\n",
    "textSequences = tokenizer.texts_to_sequences(descriptions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 27062\n",
      "\n",
      "Training set:\n",
      "\t - x_train: 108247\n",
      "\t - y_train: 108247\n",
      "\n",
      "Testing set:\n",
      "\t - x_test: 27062\n",
      "\t - y_test: 27062\n",
      "Максимальное количество слов в самом длинном описании заявки: 41 слов\n",
      "Всего уникальных слов в словаре: 59879\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_data_from_arrays(textSequences, categories, train_test_split=0.8)\n",
    "# Максимальное количество слов в самом длинном описании заявки\n",
    "max_words = 0\n",
    "for desc in descriptions.tolist():\n",
    "    words = len(desc.split())\n",
    "    if words > max_words:\n",
    "        max_words = words\n",
    "print('Максимальное количество слов в самом длинном описании заявки: {} слов'.format(max_words))\n",
    "\n",
    "total_unique_words = len(tokenizer.word_counts)\n",
    "print('Всего уникальных слов в словаре: {}'.format(total_unique_words))\n",
    "\n",
    "maxSequenceLength = max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2395"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = round(total_unique_words/25)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество категорий для классификации: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print('Количество категорий для классификации: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Преобразуем описания заявок в векторы чисел...\n"
     ]
    }
   ],
   "source": [
    "print(u'Преобразуем описания заявок в векторы чисел...')\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(descriptions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33763/2546027198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxSequenceLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxSequenceLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(X_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_train = pad_sequences(X_train, maxlen=maxSequenceLength)\n",
    "X_test = pad_sequences(X_test, maxlen=maxSequenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность X_train: (108247, 41)\n",
      "Размерность X_test: (27062, 41)\n",
      "Преобразуем категории в матрицу двоичных чисел (для использования categorical_crossentropy)\n",
      "y_train shape: (108247, 2)\n",
      "y_test shape: (27062, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Размерность X_train:', X_train.shape)\n",
    "print('Размерность X_test:', X_test.shape)\n",
    "\n",
    "print(u'Преобразуем категории в матрицу двоичных чисел '\n",
    "      u'(для использования categorical_crossentropy)')\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собираем модель...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 41)          98195     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                9472      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,733\n",
      "Trainable params: 107,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "# максимальное количество слов для анализа\n",
    "max_features = vocab_size\n",
    "\n",
    "print(u'Собираем модель...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, maxSequenceLength))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тренируем модель...\n",
      "Epoch 1/3\n",
      "3383/3383 [==============================] - 130s 37ms/step - loss: 0.3759 - accuracy: 0.8764 - val_loss: 0.3736 - val_accuracy: 0.8768\n",
      "Epoch 2/3\n",
      "3383/3383 [==============================] - 109s 32ms/step - loss: 0.3744 - accuracy: 0.8764 - val_loss: 0.3732 - val_accuracy: 0.8768\n",
      "Epoch 3/3\n",
      "1636/3383 [=============>................] - ETA: 53s - loss: 0.3745 - accuracy: 0.8763"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33763/1261371342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Тренируем модель...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history = model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m      6\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "print(u'Тренируем модель...')\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
